{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# SESSION 4 "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["TOC:<br>\n","\n","1. Face Recognition Library<br>\n","    1.1 Overview of popular face recognition libraries<br>\n","    1.2 Introduction to face detection algorithms and techniques<br>\n","    1.3 Face Encoding<br>\n","    1.4 Face Recognition<br>\n","2. Revision of numpy<br>\n","3. Revision of Open CV<br>\n","4. Revision of pandas<br>\n","5. CSV in Data Analysis<br>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","### 1. Face Recognition Library:\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1 Overview of popular face recognition libraries:\n","\n","OpenCV: <br>\n","OpenCV is a widely-used computer vision library that provides various functionalities, including face detection and recognition. It has robust support for image and video processing, making it a popular choice for face recognition tasks.<br>\n","dlib: <br>\n","dlib is a C++ library with Python bindings that offers advanced face detection and recognition capabilities. It provides pre-trained models for face landmark detection and facial feature extraction, making it useful for various face analysis tasks.<br>\n","face_recognition:<br>\n","face_recognition is a high-level face recognition library built on top of dlib. It simplifies the face recognition process by providing a simple API for face detection, face encoding, and face comparison.<br>\n","#### Installation and setup of the chosen face recognition library:\n","\n","To install a specific face recognition library, you can use the following commands:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["OPENCV : \n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (4.8.1.78)\n","Requirement already satisfied: numpy>=1.17.3 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from opencv-python) (1.23.5)\n"]}],"source":["!pip install opencv-python"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["dlib:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: dlib in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (19.24.2)\n"]}],"source":["!pip install dlib"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["face_recognition:"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: face-recognition in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (1.3.0)\n","Requirement already satisfied: dlib>=19.7 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from face-recognition) (19.24.2)\n","Requirement already satisfied: Pillow in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from face-recognition) (9.4.0)\n","Requirement already satisfied: numpy in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from face-recognition) (1.23.5)\n","Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from face-recognition) (0.3.0)\n","Requirement already satisfied: Click>=6.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from face-recognition) (8.0.4)\n","Requirement already satisfied: colorama in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from Click>=6.0->face-recognition) (0.4.6)\n"]}],"source":["!pip install face-recognition"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Introduction to the library's features and capabilities:\n","Each face recognition library offers different features and capabilities. Here's a brief overview of what you can expect from each library:\n","\n","OpenCV: OpenCV provides functions for face detection using Haar cascades or deep learning models. It also offers utilities for face recognition, such as Eigenfaces and Fisherfaces, and the ability to train custom models.\n","\n","dlib: dlib offers state-of-the-art face detection and recognition algorithms. It provides pre-trained models for face landmark detection, facial feature extraction, and face recognition. It also supports face clustering and face alignment.\n","\n","face_recognition: face_recognition library simplifies face recognition tasks by providing a high-level API. It utilizes dlib's face recognition models for face detection, encoding, and comparison. It allows you to compare faces, identify faces in images, and perform facial feature extraction.\n","\n","face_recognition: face_recognition library simplifies face recognition tasks by providing a high-level API. It utilizes dlib's face recognition models for face detection, encoding, and comparison. It allows you to compare faces, identify faces in images, and perform facial feature extraction.face_recognition: face_recognition library simplifies face recognition tasks by providing a high-level API. It utilizes dlib's face recognition models for face detection, encoding, and comparison. It allows you to compare faces, identify faces in images, and perform facial feature extraction.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","####  1.2 Introduction to face detection algorithms and techniques:\n","Face detection is a fundamental task in computer vision that involves locating and identifying faces within an image or video. Several algorithms and techniques have been developed for this purpose. Here are some commonly used ones:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Haar cascades: Haar cascades are a machine learning-based face detection algorithm. They use a cascade of simple classifiers trained on Haar-like features to detect faces. Haar-like features are rectangular regions with specific intensity patterns, such as edges and gradients. Haar cascades are fast and efficient, making them suitable for real-time face detection."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Deep learning-based detectors: Deep learning approaches have achieved significant advancements in face detection. These methods employ convolutional neural networks (CNNs) to learn discriminative features and make accurate predictions. Popular deep learning-based face detectors include the Single Shot Multibox Detector (SSD) and the You Only Look Once (YOLO) algorithm. These detectors offer higher accuracy but may be computationally more expensive."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Understanding the Haar cascades algorithm and its implementation using OpenCV:\n","Haar cascades algorithm can be implemented using the OpenCV library. Here's an example of using Haar cascades for face detection:"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import cv2"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Load the pre-trained Haar cascade XML file"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["face_cascade = cv2.CascadeClassifier('images/haarcascade_frontalface_default.xml')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Load the input image"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["image = cv2.imread('Images/input_image.jpg')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Convert the image to grayscale"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Perform face detection"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Draw bounding boxes around the detected faces"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["for (x, y, w, h) in faces:\n","    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Display the result"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["cv2.imshow('Face Detection', image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["<img align = \"middle\" src = 'Images/Output1.png' width = '700' height = '500'>"]},{"cell_type":"markdown","metadata":{},"source":["# SSD Object Detection in Real Time (Deep Learning and Caffe)\n","Utilizing deep learning-based face detectors like the SSD or YOLO algorithm: \n","Deep learning-based face detectors, such as SSD and YOLO, provide higher accuracy compared to Haar cascades but require more computational resources. Here's an example of using the SSD face detector with OpenCV's DNN module:"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we will be seeing SSD Object Detection- features, advantages, drawbacks, and implement MobileNet SSD model with Caffe â€” using OpenCV in Python."]},{"cell_type":"markdown","metadata":{},"source":["<img align = \"middle\" src = 'Images/Image1.png' width = '700' height = '300'>"]},{"cell_type":"markdown","metadata":{},"source":["### What is Object Detection?\n","\n","Object Detection in Computer Vision is as simple as it sounds- detecting and predicting objects and localizing their area. Object Detection is based on image classification. Irrespective of the latter being performed using neural networks or primitive classifiers, image classification is always the first step. Building further on this, we can perform detection which localizes all possible objects in a given frame."]},{"cell_type":"markdown","metadata":{},"source":["### Single Shot MultiBox Detector (SSD)\n","\n","SSD Object Detection extracts feature map using a base deep learning network, which are CNN based classifiers, and applies convolution filters to finally detect objects. Our implementation uses MobileNet as the base network (others might include- VGGNet, ResNet, DenseNet)."]},{"cell_type":"markdown","metadata":{},"source":["<img align = \"middle\" src = 'Images/Image2.png' width = '1000' height = '300'>"]},{"cell_type":"markdown","metadata":{},"source":["## ```MobileNet SSD  object detection using OpenCV 3.4.1 DNN module```"]},{"cell_type":"markdown","metadata":{},"source":["Let us implement how to use the OpenCV 3.4.1 deep learning module with the MobileNet-SSD network for object discovery.\n","\n","As part of Opencv 3.4. + The deep neural network (DNN) module was officially included. The DNN module allows loading pre-trained models of most popular deep learning frameworks, including Tensorflow, Caffe, Darknet, Torch. Besides MobileNet-SDD, other architectures are compatible with OpenCV 3.4.1:\n","\n","    GoogleLeNet\n","    YOLO\n","    SqueezeNet\n","    R-CNN faster\n","    ResNet\n","    This API is compatible with C ++ and Python\n"]},{"cell_type":"markdown","metadata":{},"source":["### What is Caffe?\n","\n","Caffe is a deep learning framework developed by Berkeley AI Research and community contributors. Caffe was developed as a faster and far more efficient alternative to other frameworks to perform object detection. Caffe can process 60 million images per day with a single NVIDIA K-40 GPU. That is 1 ms/image for inference and 4 ms/image for learning."]},{"cell_type":"markdown","metadata":{},"source":["####    \n","    Descripton code"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we will create the Python script for object detection and explain, how to load our deep neural network with OpenCV 3.4? How to pass the image to the neural network? and How to make a prediction with MobileNet or dnn module in OpenCV ?.\n","\n","We use a pre-trained MobileNet taken from https://github.com/chuanqi305/MobileNet-SSD/ that was trained on the Caffe-SSD framework. This model can detect 2 classes.\n","\n","Load and predict with the deep neural network module"]},{"cell_type":"markdown","metadata":{},"source":["#### Import the Libraries"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import numpy as np\n","import argparse\n","import cv2 "]},{"cell_type":"markdown","metadata":{},"source":["#### construct the argument parse "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# parser = argparse.ArgumentParser(\n","#     description='Script to run MobileNet-SSD object detection network ')\n","# parser.add_argument(\"--video\", help=\"path to video file. If empty, camera's stream will be used\")\n","# parser.add_argument(\"--prototxt\", default=\"MobileNetSSD_deploy.prototxt\",\n","#                                   help='Path to text network file: '\n","#                                        'MobileNetSSD_deploy.prototxt for Caffe model or '\n","#                                        )\n","# parser.add_argument(\"--weights\", default=\"MobileNetSSD_deploy.caffemodel\",\n","#                                  help='Path to weights: '\n","#                                       'MobileNetSSD_deploy.caffemodel for Caffe model or '\n","#                                       )\n","# parser.add_argument(\"--thr\", default=0.2, type=float, help=\"confidence threshold to filter out weak detections\")\n","# args = parser.parse_args()"]},{"cell_type":"markdown","metadata":{},"source":["#### Load the Caffe model \n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# net = cv2.dnn.readNetFromCaffe(args.prototxt, args.weights)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Capture frame-by-frame"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# ret, frame = cap.read()\n","# frame_resized = cv2.resize(frame,(300,300)) # resize frame for prediction"]},{"cell_type":"markdown","metadata":{},"source":["#### MobileNet requires fixed dimensions for input image(s)\n","    so we have to ensure that it is resized to 300x300 pixels.\n","    set a scale factor to image because network the objects has differents size. \n","    We perform a mean subtraction (127.5, 127.5, 127.5) to normalize the input;\n","    after executing this command our \"blob\" now has the shape:\n","    (1, 3, 300, 300)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["    # MobileNet requires fixed dimensions for input image(s)\n","    # so we have to ensure that it is resized to 300x300 pixels.\n","    # set a scale factor to image because network the objects has differents size. \n","    # We perform a mean subtraction (127.5, 127.5, 127.5) to normalize the input;\n","    # after executing this command our \"blob\" now has the shape:\n","    # (1, 3, 300, 300)\n","       #  blob = cv2.dnn.blobFromImage(frame_resized, 0.007843, (300, 300), (127.5, 127.5, 127.5), False)\n","    #Set to network the input blob \n","       #  net.setInput(blob)\n","    #Prediction of network\n","       #  detections = net.forward()"]},{"cell_type":"markdown","metadata":{},"source":["### Complete Code"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["person: 0.99999845\n","person: 0.9658812\n","person: 0.99999726\n","person: 0.9248693\n","chair: 0.31879643\n","person: 0.99999666\n","person: 0.91995615\n","chair: 0.33836967\n","person: 0.9999964\n","person: 0.91965336\n","chair: 0.40895724\n","person: 0.9999968\n","person: 0.94828403\n","chair: 0.3773629\n","person: 0.9999957\n","person: 0.9485657\n","person: 0.99999654\n","person: 0.9647959\n","person: 0.99999714\n","person: 0.96207786\n","person: 0.9999974\n","person: 0.98159516\n","person: 0.9999969\n","person: 0.98072493\n","person: 0.9999981\n","person: 0.97760016\n","chair: 0.2569076\n","person: 0.9999981\n","person: 0.98178333\n","person: 0.99999857\n","person: 0.97876215\n","person: 0.9999982\n","person: 0.9790496\n","person: 0.99999857\n","person: 0.98625827\n","chair: 0.28663898\n","person: 0.99999833\n","person: 0.9860406\n","chair: 0.34131056\n","person: 0.99999845\n","person: 0.99070823\n","chair: 0.26636866\n","person: 0.9999981\n","person: 0.98625064\n","chair: 0.28102937\n","person: 0.9999981\n","person: 0.9742708\n","chair: 0.3555592\n","person: 0.9999982\n","person: 0.97869337\n","chair: 0.2959008\n","person: 0.9999982\n","person: 0.9795997\n","chair: 0.35975054\n","person: 0.99999845\n","person: 0.9785993\n","chair: 0.37686333\n","person: 0.9999981\n","person: 0.9564113\n","chair: 0.37502858\n","person: 0.9999982\n","person: 0.9504963\n","chair: 0.30369422\n","person: 0.999998\n","person: 0.9508912\n","chair: 0.3677565\n","person: 0.999998\n","person: 0.9606987\n","chair: 0.32737345\n","person: 0.9999981\n","person: 0.96580005\n","chair: 0.37768632\n","person: 0.99999785\n","person: 0.9639838\n","chair: 0.35919055\n","person: 0.999998\n","person: 0.97024363\n","chair: 0.30242568\n","person: 0.99999785\n","person: 0.9762241\n","chair: 0.35003743\n","person: 0.99999785\n","person: 0.9842418\n","chair: 0.2929935\n","person: 0.99999774\n","person: 0.98365605\n","chair: 0.36459088\n","person: 0.2564383\n","person: 0.99999726\n","person: 0.9877251\n","chair: 0.33004135\n","person: 0.29024506\n","person: 0.99999714\n","person: 0.98725116\n","chair: 0.41859287\n","person: 0.27055094\n","person: 0.99999726\n","person: 0.9896797\n","chair: 0.46551087\n","person: 0.3085961\n","person: 0.9999964\n","person: 0.9927632\n","person: 0.34953803\n","chair: 0.3232757\n","person: 0.99999595\n","person: 0.99240017\n","person: 0.34489426\n","chair: 0.33905813\n","person: 0.99999607\n","person: 0.99301434\n","chair: 0.43612581\n","person: 0.41904625\n","person: 0.99999595\n","person: 0.9909733\n","person: 0.44275746\n","chair: 0.36346024\n","person: 0.99999607\n","person: 0.9904174\n","person: 0.4182858\n","chair: 0.38234773\n","person: 0.9999943\n","person: 0.959234\n","person: 0.42566812\n","chair: 0.3167486\n","person: 0.9999943\n","person: 0.96093136\n","person: 0.32814085\n","person: 0.99999166\n","person: 0.9645138\n","chair: 0.3369099\n","person: 0.321087\n","person: 0.9999907\n","person: 0.96414995\n","chair: 0.35721588\n","person: 0.33952916\n","person: 0.9999864\n","person: 0.9723112\n","person: 0.31293678\n","chair: 0.26969638\n","person: 0.99998415\n","person: 0.97757506\n","chair: 0.3168085\n","person: 0.3157822\n","person: 0.999977\n","person: 0.9854218\n","person: 0.36462188\n","chair: 0.3018313\n","person: 0.9999832\n","person: 0.9856023\n","person: 0.34240338\n","chair: 0.28004482\n","diningtable: 0.2779526\n","person: 0.99998426\n","person: 0.9853807\n","person: 0.4243424\n","diningtable: 0.28181157\n","chair: 0.25282633\n","person: 0.9999862\n","person: 0.98232377\n","person: 0.38647264\n","chair: 0.27127606\n","diningtable: 0.2672748\n","person: 0.9999864\n","person: 0.9835361\n","person: 0.34000483\n","diningtable: 0.26863265\n","person: 0.99998975\n","person: 0.97551763\n","person: 0.38678455\n","chair: 0.3150682\n","person: 0.9999894\n","person: 0.9747902\n","person: 0.43699384\n","chair: 0.34607282\n","diningtable: 0.2601914\n","person: 0.9999882\n","person: 0.9638729\n","person: 0.46269223\n","diningtable: 0.30551448\n","chair: 0.29120576\n","person: 0.9999852\n","person: 0.97126555\n","person: 0.42736387\n","chair: 0.39694953\n","diningtable: 0.3588205\n","person: 0.9999839\n","person: 0.96316683\n","person: 0.3801932\n","chair: 0.36219522\n","diningtable: 0.32837048\n","person: 0.99998426\n","person: 0.94416153\n","chair: 0.48800436\n","person: 0.3904268\n","diningtable: 0.26316404\n","person: 0.99998045\n","person: 0.93906295\n","person: 0.5199018\n","chair: 0.43543112\n","diningtable: 0.3015363\n","person: 0.9999827\n","person: 0.9441642\n","chair: 0.4300096\n","person: 0.42085302\n","diningtable: 0.31839657\n","person: 0.9999695\n","person: 0.94723284\n","chair: 0.43759102\n","diningtable: 0.37249637\n","person: 0.3535692\n","person: 0.9999751\n","person: 0.9392186\n","chair: 0.46579397\n","diningtable: 0.34882733\n","person: 0.339058\n","person: 0.9999808\n","person: 0.93658084\n","chair: 0.53437877\n","person: 0.35273832\n","diningtable: 0.33996427\n","person: 0.9999826\n","person: 0.9492988\n","chair: 0.4255999\n","diningtable: 0.35232148\n","person: 0.2917588\n","person: 0.9999809\n","person: 0.94727594\n","chair: 0.38917848\n","diningtable: 0.35939455\n","person: 0.31212157\n","person: 0.99998367\n","person: 0.9461379\n","chair: 0.4192173\n","diningtable: 0.3677801\n","person: 0.99998283\n","person: 0.944883\n","diningtable: 0.31494895\n","chair: 0.30882594\n","person: 0.26315424\n","person: 0.99999034\n","person: 0.9467055\n","chair: 0.38069308\n","diningtable: 0.2982285\n","diningtable: 0.2561001\n","person: 0.99999213\n","person: 0.9506842\n","chair: 0.44660684\n","person: 0.99999166\n","person: 0.94968003\n","chair: 0.41375157\n","person: 0.9999926\n","person: 0.95658666\n","chair: 0.4296086\n","person: 0.9999939\n","person: 0.9471407\n","chair: 0.40482348\n","diningtable: 0.2767555\n","person: 0.9999939\n","person: 0.9508483\n","chair: 0.31972232\n","diningtable: 0.28260568\n","person: 0.9999957\n","person: 0.9590769\n","chair: 0.33357716\n","diningtable: 0.27549744\n","person: 0.9999958\n","person: 0.96654373\n","chair: 0.45138735\n","person: 0.32259068\n","diningtable: 0.2751309\n","person: 0.99999464\n","person: 0.9792901\n","person: 0.53646135\n","chair: 0.42371547\n","diningtable: 0.26913017\n","person: 0.99999464\n","person: 0.98036\n","person: 0.52692634\n","chair: 0.42586288\n","diningtable: 0.28074923\n","person: 0.99999607\n","person: 0.97334665\n","chair: 0.43580702\n","person: 0.4209273\n","person: 0.9999968\n","person: 0.9747422\n","chair: 0.44320637\n","person: 0.4192263\n","diningtable: 0.25579038\n","person: 0.999997\n","person: 0.9713587\n","chair: 0.46969494\n","person: 0.37005025\n","diningtable: 0.28056997\n","person: 0.999997\n","person: 0.975764\n","person: 0.43423647\n","chair: 0.41290388\n","diningtable: 0.28673548\n","person: 0.9999962\n","person: 0.9686321\n","person: 0.42248818\n","chair: 0.41409254\n","diningtable: 0.2832665\n","person: 0.999997\n","person: 0.9729567\n","person: 0.47921023\n","chair: 0.4158182\n","diningtable: 0.29764414\n","person: 0.9999976\n","person: 0.9690136\n","person: 0.43427625\n","chair: 0.38025913\n","person: 0.9999975\n","person: 0.9727638\n","person: 0.30233133\n","chair: 0.2608078\n"]}],"source":["import numpy as np\n","import cv2\n","\n","# Set the values directly in the notebook\n","args = {\n","    \"video\": \"\",  # Set the path to the video file or leave it empty for camera stream\n","    \"prototxt\": \"Images/MobileNetSSD_deploy.prototxt\",\n","    \"weights\": \"Images/MobileNetSSD_deploy.caffemodel\",\n","    \"thr\": 0.2\n","}\n","\n","# Labels of Network.\n","classNames = {0: 'background', 1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat',\n","              5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair',\n","              10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse',\n","              14: 'motorbike', 15: 'person', 16: 'pottedplant',\n","              17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor'}\n","\n","# Open video file or capture device.\n","if args[\"video\"]:\n","    cap = cv2.VideoCapture(args[\"video\"])\n","else:\n","    cap = cv2.VideoCapture(0)\n","\n","# Load the Caffe model\n","net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"weights\"])\n","\n","while True:\n","    # Capture frame-by-frame\n","    ret, frame = cap.read()\n","    frame_resized = cv2.resize(frame, (300, 300))  # resize frame for prediction\n","\n","    # MobileNet requires fixed dimensions for input image(s)\n","    blob = cv2.dnn.blobFromImage(frame_resized, 0.007843, (300, 300), (127.5, 127.5, 127.5), False)\n","    net.setInput(blob)\n","    detections = net.forward()\n","\n","    cols = frame_resized.shape[1]\n","    rows = frame_resized.shape[0]\n","\n","    for i in range(detections.shape[2]):\n","        confidence = detections[0, 0, i, 2]\n","        if confidence > args[\"thr\"]:\n","            class_id = int(detections[0, 0, i, 1])\n","\n","            xLeftBottom = int(detections[0, 0, i, 3] * cols)\n","            yLeftBottom = int(detections[0, 0, i, 4] * rows)\n","            xRightTop = int(detections[0, 0, i, 5] * cols)\n","            yRightTop = int(detections[0, 0, i, 6] * rows)\n","\n","            heightFactor = frame.shape[0] / 300.0\n","            widthFactor = frame.shape[1] / 300.0\n","            xLeftBottom = int(widthFactor * xLeftBottom)\n","            yLeftBottom = int(heightFactor * yLeftBottom)\n","            xRightTop = int(widthFactor * xRightTop)\n","            yRightTop = int(heightFactor * yRightTop)\n","\n","            cv2.rectangle(frame, (xLeftBottom, yLeftBottom), (xRightTop, yRightTop), (0, 255, 0))\n","\n","            if class_id in classNames:\n","                label = classNames[class_id] + \": \" + str(confidence)\n","                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n","\n","                yLeftBottom = max(yLeftBottom, labelSize[1])\n","                cv2.rectangle(frame, (xLeftBottom, yLeftBottom - labelSize[1]),\n","                              (xLeftBottom + labelSize[0], yLeftBottom + baseLine),\n","                              (255, 255, 255), cv2.FILLED)\n","                cv2.putText(frame, label, (xLeftBottom, yLeftBottom),\n","                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n","\n","                print(label)\n","\n","    cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL)\n","    cv2.imshow(\"frame\", frame)\n","    if cv2.waitKey(1) >= 0:  # Break with ESC\n","        break\n","\n","# Release the video capture object and destroy all windows\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["<img align = \"middle\" src = 'Images/Image4.png' width = '800' height = '500'>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.3 Face Encoding:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Overview of face encoding and feature extraction methods:\n","Face encoding, also known as face embedding, is the process of representing a face as a numerical feature vector. This feature vector captures the unique characteristics of a face and can be used for various face recognition tasks. Here's an overview of face encoding methods:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Geometric-based methods: These methods analyze the geometric properties and spatial relationships between facial landmarks or key points. Features such as distances, angles, or ratios are extracted based on these properties. Statistical models like Principal Component Analysis (PCA) or Linear Discriminant Analysis (LDA) can be applied to further reduce the dimensionality of the features."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Deep learning-based methods: Deep learning approaches have demonstrated remarkable success in face encoding. These methods utilize deep neural networks to directly learn discriminative features from raw face images. By leveraging the power of convolutional neural networks (CNNs), deep learning-based face encoders can capture complex patterns and variations in face appearance."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Introduction to facial landmarks detection and its importance in face encoding:\n","Facial landmarks detection involves identifying key points on a face, such as the corners of the eyes, nose, and mouth. It is important in face encoding because:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Alignment: Facial landmarks can be used to align faces to a standardized pose. By aligning faces, variations due to head pose or facial expression can be minimized, resulting in more accurate feature extraction."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Region of Interest (ROI) selection: Facial landmarks provide information about the structure and shape of a face. They can be used to define a specific region of interest, such as the eyes or mouth, for extracting relevant features."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Using facial landmarks to align faces for accurate feature extraction:\n","To align faces using facial landmarks, you can follow these steps:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Detect facial landmarks using a facial landmarks detection algorithm. Popular libraries like dlib and OpenCV provide pre-trained models for this purpose."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Select specific facial landmarks that define the alignment transformation. For example, you can use the corners of the eyes or the tip of the nose."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Calculate the transformation parameters, such as rotation, translation, and scaling, based on the selected landmarks."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Apply the calculated transformation to align the face to a standardized pose."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Introduction to deep learning-based face encoders like FaceNet, ArcFace, or dlib:\n","Deep learning-based face encoders are powerful methods for extracting face embeddings. Here are brief introductions to some popular face encoding algorithms:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["FaceNet: FaceNet is a deep learning-based face recognition model that learns a 128-dimensional embedding for each face. It uses a triplet loss function during training to optimize the embedding space, ensuring that the embeddings of the same person are close together while those of different people are far apart."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["ArcFace: ArcFace is a state-of-the-art deep learning-based face recognition model that introduces an angular margin to the traditional softmax loss. This margin-based loss function enhances the discriminative power of the learned embeddings."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["dlib: The dlib library provides a pre-trained face recognition model that combines deep learning with geometric-based alignment. It computes a 128-dimensional face embedding using a neural network trained on a large dataset."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Extracting facial embeddings or feature vectors using the chosen face encoding algorithm:\n","To extract facial embeddings using a chosen face encoding algorithm, you can follow these steps:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Detect and align the face using facial landmarks detection techniques.\n","Preprocess the aligned face image, such as resizing and normalization, to match the requirements of the face encoding model."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Pass the preprocessed face image through the face encoding model to obtain the face embedding. The output will be a high-dimensional feature vector that represents the unique characteristics of the face."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Store the extracted face embeddings in a database or use them for face recognition tasks, such as face identification or verification."]},{"cell_type":"markdown","metadata":{},"source":["```Here's an example of extracting facial embeddings using the dlib library:```"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import dlib\n","import cv2\n","\n","# Load the image\n","image = cv2.imread(\"Images/input_image.jpg\")\n","image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for dlib\n","\n","# Load the face landmarks predictor\n","predictor_path = \"Images/shape_predictor_68_face_landmarks.dat\"  # Replace with the actual path\n","predictor = dlib.shape_predictor(predictor_path)\n","\n","# Load the face detector\n","detector = dlib.get_frontal_face_detector()\n","\n","# Detect faces in the image\n","faces = detector(image_rgb)\n","\n","# Iterate over detected faces\n","for face in faces:\n","    # Predict face landmarks\n","    landmarks = predictor(image_rgb, face)\n","    landmarks_list = [(p.x, p.y) for p in landmarks.parts()]\n","\n","    # Draw landmarks on the image\n","    for landmark in landmarks_list:\n","        cv2.circle(image, landmark, 1, (0, 255, 0), -1)\n","\n","# Display the image with landmarks\n","cv2.imshow(\"Image with Face Landmarks\", image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","metadata":{},"source":["<img align = \"middle\" src = 'Images/Image5.png' width = '900' height = '500'>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### ```1.4 Face Recognition:```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Understanding the concept of face recognition and its applications:\n","Face recognition is a technology that identifies or verifies individuals by analyzing and comparing their facial features. It has various applications, including:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Access control: Face recognition can be used for secure authentication and access control in systems such as door entry systems or mobile devices."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Surveillance: Face recognition can aid in identifying individuals in surveillance videos or images, assisting law enforcement agencies in criminal investigations."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Personalization: Face recognition can enable personalized experiences in applications like social media, e-commerce, or personalized advertising."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Creating a face recognition pipeline using the selected face recognition library:\n","To create a face recognition pipeline using a chosen face recognition library, you can follow these steps:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Gather a labeled dataset: Collect a dataset of face images with corresponding labels or identities."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Preprocess the images: Perform necessary preprocessing steps, such as face detection, alignment, resizing, and normalization, to ensure consistent input for the face recognition model."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Train a face recognition model: Utilize the labeled dataset to train a face recognition model. This involves extracting facial features or embeddings and training a classifier or distance-based algorithm for identification or verification."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Implement face identification: Create a function that takes an input face image and compares it with the known faces in the database to identify the person."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Implement face verification: Develop a function that takes an input face image and a claimed identity and determines if the face matches the claimed identity."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Training a face recognition model with a labeled dataset:\n","Here's an example of training a face recognition model using the face_recognition library in Python:"]},{"cell_type":"markdown","metadata":{},"source":["### Explanation of the Face Recognition Model in individual steps"]},{"cell_type":"markdown","metadata":{},"source":["Import Libraries"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import os\n","import face_recognition\n","import numpy as np\n","import joblib\n","from PIL import UnidentifiedImageError"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Path to the labeled dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["dataset_path = \"Images/\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Load the dataset"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Load images and compute face encodings\n","image_paths = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n","known_encodings = []\n","labels = []"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Extract face encodings and labels from the dataset"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No faces found in: Images/Image1.png\n","No faces found in: Images/Image2.png\n","No faces found in: Images/Image3.png\n"]}],"source":["for image_path in image_paths:\n","    try:\n","        image = face_recognition.load_image_file(image_path)\n","        face_encodings = face_recognition.face_encodings(image)\n","        \n","        # Check if at least one face is found\n","        if face_encodings:\n","            encoding = face_encodings[0]  # Assuming a single face in each image\n","            label = os.path.splitext(os.path.basename(image_path))[0]\n","            known_encodings.append(encoding)\n","            labels.append(label)\n","        else:\n","            print(f\"No faces found in: {image_path}\")\n","    except UnidentifiedImageError as e:\n","        print(f\"Skipping non-image file: {image_path}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Train a face recognition model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Save the face recognition model\n","face_recognition_model = {\n","    'known_encodings': known_encodings,\n","    'labels': labels\n","}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Save the trained model for later use"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["['Images/face_recognition_model.pkl']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["joblib.dump(face_recognition_model, \"Images/face_recognition_model.pkl\")\n","\n","# Techniques for face identification and verification:\n","# For face identification and verification, you can use techniques like k-Nearest Neighbors (k-NN) or distance-based algorithms. Here's an example using the face_recognition library:"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import face_recognition"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Load the trained model"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Load the face recognition model\n","loaded_model = joblib.load(\"Images/face_recognition_model.pkl\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Identify a face"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def identify_face(input_image):\n","    try:\n","        input_encodings = face_recognition.face_encodings(input_image)\n","        \n","        # Check if at least one face is found\n","        if not input_encodings:\n","            print(\"No faces found in the input image.\")\n","            return None\n","\n","        input_encoding = input_encodings[0]  # Assuming a single face in the input image\n","        \n","        # Compare the face encoding with the known encodings\n","        distances = face_recognition.face_distance(loaded_model['known_encodings'], input_encoding)\n","        min_distance_index = np.argmin(distances)\n","        identified_label = loaded_model['labels'][min_distance_index]\n","        \n","        return identified_label\n","    \n","    except UnidentifiedImageError as e:\n","        print(f\"Error processing input image: {e}\")\n","        return None"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Verify a face"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def verify_face(input_image, claimed_identity):\n","    input_encoding = face_recognition.face_encodings(input_image)[0]  # Assuming a single face in the input image\n","    \n","    # Compare the face encoding with the known encodings\n","    distances = face_recognition.face_distance(loaded_model['known_encodings'], input_encoding)\n","    min_distance = np.min(distances)\n","    \n","    # Set a threshold for verification\n","    threshold = 0.6\n","    \n","    if min_distance <= threshold:\n","        return claimed_identity\n","    else:\n","        return \"Verification failed\"\n","\t\t\n","\t\t\n","# Evaluating the performance of the face recognition system, including metrics like accuracy, precision, and recall:\n","# To evaluate the performance of a face recognition system, you can calculate various metrics:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Function to evaluate the face recognition system"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Evaluate the performance of the face recognition system\n","def evaluate_system():\n","    true_positives = 0\n","    true_negatives = 0\n","    false_positives = 0\n","    false_negatives = 0\n","    \n","    for image_path in image_paths:\n","        try:\n","            print(\"Inside Try\")\n","            image = face_recognition.load_image_file(image_path)\n","            print(\"1\")\n","            label = os.path.splitext(os.path.basename(image_path))[0]\n","            print(\"2\")\n","            # Identify the face\n","            identified_label = identify_face(image)\n","            print(\"identified_label\",identified_label)\n","            # Calculate metrics\n","            if label == identified_label:\n","                if label == claimed_identity:\n","                    true_positives += 1\n","                else:\n","                    true_negatives += 1\n","            else:\n","                if label == claimed_identity:\n","                    false_negatives += 1\n","                else:\n","                    false_positives += 1\n","        except UnidentifiedImageError as e:\n","            print(f\"Skipping non-image file: {image_path}\")\n","\n","    # Calculate accuracy, precision, and recall\n","    accuracy = (true_positives + true_negatives) / len(image_paths)\n","    precision = true_positives / (true_positives + false_positives)\n","    recall = true_positives / (true_positives + false_negatives)\n","    \n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","\t\n","# In this example, the evaluate_system() function compares the identified labels with the true labels and calculates metrics such as accuracy, precision, and recall."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Note: The specific implementation and choice of metrics may vary based on your requirements and the chosen face recognition library or model."]},{"cell_type":"markdown","metadata":{},"source":["### Complete Code"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No faces found in: Images/Image1.png\n","No faces found in: Images/Image2.png\n","No faces found in: Images/Image3.png\n","Running identify_face()\n","Running verify_face()\n","Identified Person: James\n","Verification Result: James\n","Running evaluate_system()\n","Running Loop with Images\n","Running identify_face()\n","No faces found in the input image.\n","identified_label None\n","Running Loop with Images\n","Running identify_face()\n","No faces found in the input image.\n","identified_label None\n","Running Loop with Images\n","Running identify_face()\n","No faces found in the input image.\n","identified_label None\n","Running Loop with Images\n","Running identify_face()\n","identified_label Image4\n","Running Loop with Images\n","Running identify_face()\n","identified_label Image5\n","Running Loop with Images\n","Running identify_face()\n","identified_label input_image\n","Running Loop with Images\n","Running identify_face()\n","identified_label James\n","Running Loop with Images\n","Running identify_face()\n","identified_label Output1\n","Running Loop with Images\n","Running identify_face()\n","identified_label James\n","Accuracy: 0.5555555555555556\n","Precision: 0.2\n","Recall: 1.0\n"]}],"source":["import os\n","import face_recognition\n","import numpy as np\n","import joblib\n","from PIL import UnidentifiedImageError\n","\n","dataset_path = \"Images/\"\n","\n","# Load images and compute face encodings\n","image_paths = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n","known_encodings = []\n","labels = []\n","\n","for image_path in image_paths:\n","    try:\n","        image = face_recognition.load_image_file(image_path)\n","        face_encodings = face_recognition.face_encodings(image)\n","        \n","        # Check if at least one face is found\n","        if face_encodings:\n","            encoding = face_encodings[0]  # Assuming a single face in each image\n","            label = os.path.splitext(os.path.basename(image_path))[0]\n","            known_encodings.append(encoding)\n","            labels.append(label)\n","        else:\n","            print(f\"No faces found in: {image_path}\")\n","    except UnidentifiedImageError as e:\n","        print(f\"Skipping non-image file: {image_path}\")\n","\n","# Save the face recognition model\n","face_recognition_model = {\n","    'known_encodings': known_encodings,\n","    'labels': labels\n","}\n","\n","joblib.dump(face_recognition_model, \"Images/face_recognition_model.pkl\")\n","\n","# Load the face recognition model\n","loaded_model = joblib.load(\"Images/face_recognition_model.pkl\")\n","\n","def identify_face(input_image):\n","    try:\n","        print(\"Running identify_face()\")\n","        input_encodings = face_recognition.face_encodings(input_image)\n","        \n","        # Check if at least one face is found\n","        if not input_encodings:\n","            print(\"No faces found in the input image.\")\n","            return None\n","\n","        input_encoding = input_encodings[0]  # Assuming a single face in the input image\n","        \n","        # Compare the face encoding with the known encodings\n","        distances = face_recognition.face_distance(loaded_model['known_encodings'], input_encoding)\n","        min_distance_index = np.argmin(distances)\n","        identified_label = loaded_model['labels'][min_distance_index]\n","        \n","        return identified_label\n","    \n","    except UnidentifiedImageError as e:\n","        print(f\"Error processing input image: {e}\")\n","        return None\n","\n","def verify_face(input_image, claimed_identity):\n","    print(\"Running verify_face()\")\n","    input_encoding = face_recognition.face_encodings(input_image)[0]  # Assuming a single face in the input image\n","    \n","    # Compare the face encoding with the known encodings\n","    distances = face_recognition.face_distance(loaded_model['known_encodings'], input_encoding)\n","    min_distance = np.min(distances)\n","    \n","    # Set a threshold for verification\n","    threshold = 0.6\n","    \n","    if min_distance <= threshold:\n","        return claimed_identity\n","    else:\n","        return \"Verification failed\"\n","\n","# Evaluate the performance of the face recognition system\n","def evaluate_system():\n","    print(\"Running evaluate_system()\")\n","    true_positives = 0\n","    true_negatives = 0\n","    false_positives = 0\n","    false_negatives = 0\n","    \n","    for image_path in image_paths:\n","        try:\n","            print(\"Running Loop with Images\")\n","            image = face_recognition.load_image_file(image_path)\n","            \n","            label = os.path.splitext(os.path.basename(image_path))[0]\n","            \n","            # Identify the face\n","            identified_label = identify_face(image)\n","            print(\"identified_label\",identified_label)\n","            # Calculate metrics\n","            if label == identified_label:\n","                if label == claimed_identity:\n","                    true_positives += 1\n","                else:\n","                    true_negatives += 1\n","            else:\n","                if label == claimed_identity:\n","                    false_negatives += 1\n","                else:\n","                    false_positives += 1\n","        except UnidentifiedImageError as e:\n","            print(f\"Skipping non-image file: {image_path}\")\n","\n","    # Calculate accuracy, precision, and recall\n","    accuracy = (true_positives + true_negatives) / len(image_paths)\n","    precision = true_positives / (true_positives + false_positives)\n","    recall = true_positives / (true_positives + false_negatives)\n","    \n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","\n","# Example usage\n","claimed_identity = \"James\"\n","test_image_path = \"Images/James.jpg\"\n","test_image = face_recognition.load_image_file(test_image_path)\n","\n","identified_person = identify_face(test_image)\n","verification_result = verify_face(test_image, claimed_identity)\n","\n","print(\"Identified Person:\", identified_person)\n","print(\"Verification Result:\", verification_result)\n","\n","# Evaluate the system\n","evaluate_system()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Revision of numpy\n","Refer to Level 1 - Session 1 & 2 "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. Revision of Open CV\n","Refer to Level 1 - Session 3"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4. Revision of pandas\n","\n","Refer to Level 1 - Session 4"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5. CSV in Data Analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["What is CSV (Comma-Separated Values)?\n","CSV (Comma-Separated Values) is a plain-text file format commonly used for storing and exchanging tabular data. It uses commas to separate values within each row and newline characters to separate rows. CSV files are simple and widely supported, making them a popular choice for data storage and exchange."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Structure and characteristics of CSV files:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Each row represents a data record, and each column represents a data field.\n","Values within each row are separated by commas.\n","The first row often contains column headers.\n","CSV files are plain text files and can be opened with any text editor.\n","CSV files have a flat structure and do not support nested or hierarchical data.\n","Advantages and common use cases of CSV in data analysis:\n","Advantages:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["CSV files are human-readable and platform-independent.\n","They can be easily created, edited, and processed using various software tools.\n","CSV files have a small file size compared to other file formats like Excel.\n","They are widely supported by programming languages and data analysis libraries.\n","Common use cases:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importing and exporting data between different software applications and databases.\n","Storing and sharing structured data in a simple and portable format.\n","Conducting data analysis and exploratory data analysis (EDA).\n","Creating datasets for machine learning and statistical modeling.\n","Reading CSV files using built-in Python libraries (e.g., csv module, pandas library):\n","Reading CSV using the csv module:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Writing from lists using the csv module:"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import csv\n","import pandas as pd\n","\n","data = [['Name', 'Age'], ['John', 25], ['Alice', 30], ['Bob', 35]]\n","\n","# Writing to CSV using the csv module\n","with open('Images/new1.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerows(data)\n","\n","# Writing from a Pandas DataFrame\n","columns = data[0]\n","rows = data[1:]\n","\n","df = pd.DataFrame(rows, columns=columns)\n","df.to_csv('Images/new2.csv', index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":2}
