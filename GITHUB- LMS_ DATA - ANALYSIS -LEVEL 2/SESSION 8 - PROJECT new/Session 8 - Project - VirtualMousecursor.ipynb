{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SESSION 8 - Project Virtual Mouse Cursor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOC : <br>\n",
    "\n",
    "1. Introduction\n",
    "2. Importing Packages\n",
    "3. Project Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the necessary packages and Convention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align = \"middle\" src = 'images/ai-image-virtual-mouse.jpg' width = '600' height = auto>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The mouse is the one of the wonderfull inventions of the Human Computer Interaction(HCI) Technology.\n",
    "\n",
    "Currently we are using wired or may be wireless mouses,In real time cases some computers may not support for a physical mouse or may some users may be dealth with some hand problems or handicap and cannot use physical mouse, so this Hand controlled AI Virtual Mouse can be used to overcome this problem. Making a user to control the mouse by reducing the computer human interaction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align = \"middle\" src = 'images/output-image-omotec.png' width = '700' height = auto>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTIONALITIES:\n",
    "\n",
    "- This was built using the openCV-python and mediapipe for Detecting and processing the image and mediapipe an open source cross-platform developed by google for media processing and ready-to-use ML solutions for computer vision tasks.\n",
    "- Our Hand Controlled Virtual mouse can able to move the mouse anywhere on the screen and can able to perform the click operation.\n",
    "- Our Index finger can be used to move the mouse over the screen.\n",
    "- When Our Index finger and Thumb come close to each other or touch each other then it performs the click operation.\n",
    "- And the PyautoGUI for programmatically control the mouse and keyboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install the following necessary pip\n",
    "```\n",
    "- pip install opencv-python\n",
    "- pip install numpy\n",
    "- pip install matplotlib\n",
    "- pip install mediapipe\n",
    "- pip install PyautoGUI\n",
    "- pip install PIL\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import ht\n",
    "import cv2  # pip install opencv-contrib-python\n",
    "import numpy as np\n",
    "import mediapipe as mp  # pip install mediapipe\n",
    "import pyautogui    # pip install PyautoGUI\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/hand_landmarks.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = Image(url=\"images/hand_landmarks.png\")\n",
    "points\n",
    "# img=cv2.imread(\"hand_landmarks.png\")\n",
    "# RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)   # capture video '0' one cam\n",
    "hand_detector = mp.solutions.hands.Hands()  # detect hand\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "# print(screen_width, screen_height)\n",
    "index_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Smoothen the movement of mouse to stop at the exact position of,\n",
    "   our hand movement without any shake in the movement of the mouse'''\n",
    "smoothening = 9\n",
    "plocx, plocy = 0, 0\n",
    "clocx, clocy = 0, 0 \n",
    "x1,y1 = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    _, frame = cap.read()   # read data from cap\n",
    "    '''Flip the frame or screen since the camera shows the mirror image,\n",
    "       of our hand and moves in opposite direction so we need to flip the screen'''\n",
    "    frame = cv2.flip(frame, 1)\n",
    "     # shape gives frame height and width using shape \n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # detect on rgb frame color\n",
    "    output = hand_detector.process(rgb_frame)\n",
    "    hands = output.multi_hand_landmarks # hand landmark\n",
    "    \n",
    "    if hands:\n",
    "        for hand in hands:\n",
    "            drawing_utils.draw_landmarks(frame, hand)   # see landmarks on frame\n",
    "            landmarks = hand.landmark\n",
    "            # we use our index finger tip move the mouse \n",
    "    \n",
    "            for id, landmark in enumerate(landmarks):   # add counter\n",
    "                # show the landmarks on kernel in x and y axis\n",
    "                # x and y axis is multiplies by the height and width to get the x and y axis on the frames\n",
    "                x = int(landmark.x*frame_width)\n",
    "                y = int(landmark.y*frame_height)\n",
    "                # Index finger tip point number is 8\n",
    "                # and draw a boundary to the point a circle\n",
    "                if id == 8:\n",
    "                    cv2.circle(img=frame, center=(x,y), radius=15, color=(0, 255, 255))\n",
    "                    # pyautogui.moveTo(x,y)\n",
    "                    index_x = (screen_width/frame_width)*x\n",
    "                    index_y = (screen_height/frame_height)*y\n",
    "                    # co-ordinates need to be changed \n",
    "                    # smoothining varies with the change in the smoothening factor\n",
    "                    clocx = plocx + ( index_x - plocx ) / smoothening\n",
    "                    clocy = plocy + ( index_y - plocy ) / smoothening\n",
    "                    pyautogui.moveTo(clocx, clocy)\n",
    "                    plocx, plocy, x1, y1 = clocx, clocy, landmark.x, landmark.y\n",
    "                \n",
    "                # thumb tip point number is 4\n",
    "\n",
    "                if id == 4:\n",
    "                    cv2.circle(img=frame, center=(x,y), radius=15, color=(0, 255, 255))\n",
    "                    thumb_x = (screen_width/frame_width)*x\n",
    "                    thumb_y = (screen_height/frame_height)*y\n",
    "                    print('distance : ', abs(index_y - thumb_y))\n",
    "                    if abs(index_y - thumb_y) < 70:\n",
    "                        print('click')\n",
    "                        pyautogui.click()\n",
    "                        pyautogui.sleep(1)\n",
    "    cv2.imshow('Virtual Mouse', frame)  # show image\n",
    "    cv2.waitKey(1)  # waits for key infinitely\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout the Demo Below :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align = \"middle\" src = 'images/output_omotec.gif' width = '700' height = auto>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa6820bafbacf8424655be4f036d97fe1bf53e85f3476c591555871e686f57fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
