{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2 - MediaPipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOC : <br>\n",
    "\n",
    "1. Overview of MediaPipe and its Capabilities\n",
    "2. Comparison with other computer vision libraries and frameworks\n",
    "3. Installation and Setup\n",
    "4. MediaPipe Hand tracking and Gesture recognition\n",
    "5. MediaPipe Face Detection and Tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview of MediaPipe and its Capabilities\n",
    "\n",
    "MediaPipe is an open-source cross-platform framework developed by Google for building multimodal machine learning applications. It provides a wide range of pre-built modules for tasks like face detection, pose estimation, hand tracking, object detection, and more. MediaPipe is built using C++ and Python and can be used on multiple platforms like Android, iOS, Windows, and Linux. MediaPipe also provides APIs for integrating custom machine learning models into the pipeline.\n",
    "\n",
    "Mediapipe is a powerful library that provides a wide range of computer vision and machine learning solutions. Here are some of the things that can be done using Mediapipe:\n",
    "\n",
    "- Object detection and tracking: Detecting and tracking objects in video or images.\n",
    "- Face detection and recognition: Detecting faces and recognizing faces in video or images.\n",
    "- Hand tracking and gesture recognition: Detecting and tracking hands and recognizing gestures in real-time video or images.\n",
    "- Pose estimation: Estimating the human body pose from images or video streams.\n",
    "- Segmentation: Segmenting objects from images or video streams.\n",
    "Image and video processing: Various image and video processing operations such as resizing, cropping, rotation, filtering, and blending.\n",
    "- Audio processing: Processing and analyzing audio signals for various applications such as speech recognition, speaker identification, and emotion detection.\n",
    "- Natural Language Processing (NLP): Natural language processing tasks such as sentiment analysis, text classification, and speech-to-text conversion.\n",
    "These are just a few examples of the many things that can be done using Mediapipe. The library is constantly evolving and new features are added frequently, making it an extremely versatile and powerful tool for computer vision and machine learning applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comparison with other computer vision libraries and frameworks\n",
    "\n",
    "MediaPipe provides a unique combination of machine learning-based approaches and traditional computer vision techniques that make it stand out from other computer vision libraries like OpenCV and frameworks like TensorFlow. MediaPipe provides a pipeline for building multimodal applications that integrate multiple machine learning and computer vision techniques. It also provides pre-built modules that can be used out of the box, reducing the need for complex code development."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Installation and Setup\n",
    "\n",
    "MediaPipe can be installed using pip, the Python package manager, as follows:\n",
    "\n",
    "```pip install mediapipe```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (0.10.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (1.23.5)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (3.7.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: pycparser in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MediaPipe Hand tracking and Gesture recognition\n",
    "\n",
    "One of the most popular features of Mediapipe is the hand landmark detection module, which allows for real-time and accurate detection of 21 key points (landmarks) on a person's hand.\n",
    "\n",
    "The hand landmark detection module uses a deep neural network to analyze an input image or video frame and predict the 3D coordinates of the 21 hand landmarks. These landmarks correspond to various points on the hand, such as the tips of the fingers, the base of the thumb, and the center of the palm.\n",
    "\n",
    "The Mediapipe hand landmark detection pipeline is composed of several stages, including:\n",
    "\n",
    "- Hand detection: The first step involves detecting the presence of a hand in the input image or video frame. This is done using a machine learning model that has been trained to recognize the shape and structure of a hand.\n",
    "\n",
    "- Hand localization: Once a hand has been detected, the next step involves localizing the hand and aligning it to a canonical coordinate system. This is important for ensuring that the hand landmarks are consistently detected across different orientations and positions of the hand.\n",
    "\n",
    "- Hand landmark estimation: The final stage involves estimating the 3D coordinates of the 21 hand landmarks. This is done using a deep neural network that has been trained on a large dataset of hand images and corresponding landmark annotations.\n",
    "\n",
    "Once the hand landmarks have been detected, they can be used for a wide range of applications, such as gesture recognition, hand tracking, and virtual try-on. The Mediapipe hand landmark detection module is highly optimized for real-time performance and can be easily integrated into Python applications using the Mediapipe Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load the Mediapipe hand landmark model\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# Initialize the drawing module for hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame from the video capture object\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the color space from BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect the hand landmarks in the current frame\n",
    "    results = mp_hands.process(frame)\n",
    "\n",
    "    # Draw the hand landmarks and connections on the current frame\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the current frame in a window\n",
    "    cv2.imshow('Hand Landmarks', frame)\n",
    "\n",
    "    # Check for a key event and exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img align = \"middle\" src = 'Images/Output1.png' width = '700' height = '500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MediaPipe Face Detection and Tracking\n",
    "\n",
    "- MediaPipe Face Detection and Tracking is a pre-built computer vision pipeline developed by Google that uses machine learning to detect and track faces in real-time video streams or image sequences. It is based on a deep neural network trained on a large dataset of images and is capable of detecting and tracking multiple faces simultaneously.\n",
    "\n",
    "- The MediaPipe Face Detection and Tracking pipeline consists of two main components: a face detection model and a face tracking model. The face detection model is responsible for detecting faces in the input video frames or images, while the face tracking model is responsible for tracking the detected faces across frames and maintaining their identities.\n",
    "\n",
    "- The face detection model is based on the Single Shot Detector (SSD) architecture, which is a popular object detection algorithm that uses a single neural network to predict object bounding boxes and class probabilities in an input image. The SSD architecture is trained on a large dataset of annotated images of faces and is capable of detecting faces in various orientations and lighting conditions.\n",
    "\n",
    "- The MediaPipe Face Detection and Tracking pipeline can be used for a wide range of applications, including video conferencing, virtual makeup try-on, and emotion detection. It is also highly customizable, allowing developers to fine-tune the pipeline for specific use cases and integrate it into their own applications.\n",
    "\n",
    "Here's an example code snippet to detect and track faces using MediaPipe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize the MediaPipe face detection module\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# Initialize the MediaPipe drawing module\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the VideoCapture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop over the frames\n",
    "while True:\n",
    "    # Read the frame from the camera\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        results = face_detection.process(img_rgb)\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Draw the bounding box around the face\n",
    "                mp_draw.draw_detection(img, detection)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow(\"Face Detection\", img)\n",
    "\n",
    "    # Wait for a key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and destroy the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img align = \"middle\" src = 'Images/Output2.png' width = '700' height = '500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. MediaPipe Hand Gesture Recognition on Automatic Volume Control Project\n",
    "\n",
    "In this Python project, we are trying to process a video so that we can control volume of device with help of webcam camera using the tip our index finger.\n",
    "\n",
    "\n",
    "Gesture recognition helps computers to understand human body language. This helps to build a more potent link between humans and machines, rather than just the basic text user interfaces or graphical user interfaces (GUIs). In this project for gesture recognition, the human body’s motions are read by computer camera. The computer then makes use of this data as input to handle applications. The objective of this project is to develop an interface which will capture human hand gesture dynamically and will control the volume level.\n",
    "\n",
    "1) NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "2) Pycaw : Python Audio Control Library\n",
    "\n",
    "3) Mediapipe is an open-source machine learning library of Google, which has some solutions for face recognition and gesture recognition, and provides encapsulation of python, js and other languages. MediaPipe Hands is a high-fidelity hand and finger tracking solution. It uses machine learning (ML) to infer 21 key 3D hand information from just one frame. We can use it to extract the coordinates of the key points of the hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img align = \"middle\" src = 'Images/GIF-Yash-Datar-Mediapipe-Project.gif' width = '700' height = '500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pip install is required for the code you provided:\n",
    "```pip install mediapipe pycaw numpy comtypes```\n",
    "OR\n",
    "\n",
    "```pip install mediapipe –upgrade```\n",
    "\n",
    "```pip install pycaw –upgrade```\n",
    "\n",
    "```pip install comtypes –upgrade``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comtypes in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (0.10.5)\n",
      "Requirement already satisfied: pycaw in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (20230407)\n",
      "Requirement already satisfied: numpy in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (3.7.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from pycaw) (5.9.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\omolp091\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install comtypes mediapipe pycaw numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import necessary libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from math import hypot\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import numpy as np\n",
    "\n",
    "##### Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "##### Initialize MediaPipe Hands\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands()\n",
    "##### Initialize MediaPipe Drawing Utilities\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "### Python code To access speaker through the library pycaw\n",
    "##### Access speaker through the library pycaw\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "##### Set initial volume bar position and percentage\n",
    "volbar = 400\n",
    "volper = 0\n",
    "\n",
    "\n",
    "volMin, volMax = volume.GetVolumeRange()[:2]\n",
    "    \n",
    "    \n",
    "while True:\n",
    "  success, img = cap.read()\n",
    "\n",
    "  imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  ### Python code to Collection of gesture information\n",
    "  results = hands.process(imgRGB)\n",
    "\n",
    "  lmList = []\n",
    "  ##### If hands are detected\n",
    "  if results.multi_hand_landmarks:\n",
    "    ##### Iterate over all detected hands\n",
    "    for handlandmark in results.multi_hand_landmarks:\n",
    "      ##### Iterate over all landmarks in the hand\n",
    "      for id, lm in enumerate(handlandmark.landmark):\n",
    "        ### Python code to Get finger joint points\n",
    "        h, w, _ = img.shape\n",
    "        cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "        ##### Add landmark to list\n",
    "        lmList.append([id, cx, cy])\n",
    "      ##### Draw landmarks on image\n",
    "      mpDraw.draw_landmarks(img, handlandmark, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    " \n",
    "    ##### If any landmarks were found\n",
    "    if lmList != []:\n",
    "        ##### Get coordinates of palm base and index finger tip\n",
    "        #getting the value at a point\n",
    "                        #x      #y\n",
    "        x1,y1 = lmList[0][1],lmList[0][2]  #palm\n",
    "        x2,y2 = lmList[8][1],lmList[8][2]  #index finger\n",
    "        \n",
    "        ### Python code to creating circle at the tips of thumb and index finger\n",
    "        cv2.circle(img,(x1,y1),13,(0,0,255),cv2.FILLED) #image #fingers #radius #rgb\n",
    "        cv2.circle(img,(x2,y2),13,(255,0,0),cv2.FILLED) #image #fingers #radius #rgb\n",
    "        cv2.line(img,(x1,y1),(x2,y2),(0,255,0),3)  #create a line b/w tips of index finger and thumb\n",
    "        \n",
    "        ### Python code to add a LIGHT BLUE box for INDEX FINGER MEDIAPIPE hand landmarks +8\n",
    "        # Get the index finger landmark.\n",
    "        index_finger_landmark = lmList[8]\n",
    "\n",
    "        ### Python code to Calculate the top-left and bottom-right coordinates of the box.\n",
    "        box_top_left = (index_finger_landmark[1] - 10, index_finger_landmark[2] - 10)\n",
    "        box_bottom_right = (index_finger_landmark[1] + 10, index_finger_landmark[2] + 10)\n",
    "\n",
    "        # Draw the box.\n",
    "        cv2.rectangle(img, box_top_left, box_bottom_right, (255, 255, 0), 2)\n",
    "  \n",
    "  \n",
    "  \n",
    "        ### Python code to Calculate the distance between palm base and index finger tip.\n",
    "        length = hypot(x2-x1,y2-y1) #distance b/w tips using hypotenuse\n",
    "        # from numpy we find our length,by converting hand range in terms of volume range ie b/w -63.5 to 0\n",
    "        vol = np.interp(length,[30,350],[volMin,volMax]) \n",
    "        volbar=np.interp(length,[30,350],[400,150])\n",
    "        volper=np.interp(length,[30,350],[0,100])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Python code to display_vol_in_reverse(vol)\n",
    "        # Convert the values in the array to a range of 0 to 100, with 0 displayed as 100, 1 as 99, etc. till 100 as 0.\n",
    "        # Convert the input from negative to positive and positive to negative.\n",
    "        vols = -np.abs(vol)\n",
    "        vols = (100 + vols)\n",
    "        print(\"Volume => \",vols, \" Length => \",int(length))\n",
    "        volume.SetMasterVolumeLevel(vol, None)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Hand range 30 - 350\n",
    "        # Volume range -63.5 - 0.0\n",
    "        ### Python code to creating volume bar for volume level \n",
    "        cv2.rectangle(img,(50,150),(85,400),(0,0,255),4) # vid ,initial position ,ending position ,rgb ,thickness\n",
    "        cv2.rectangle(img,(50,int(volbar)),(85,400),(0,0,255),cv2.FILLED)\n",
    "        #RK cv2.putText(img,f\"{int(volper)}%\",(10,100),cv2.FONT_ITALIC,1,(0, 255, 98),3)\n",
    "        cv2.putText(img,f\"Volume Level : {int(vols)}\",(10,40),cv2.FONT_ITALIC,1,(255, 0, 0),3)\n",
    "        \n",
    "        ### Python code to tell the volume percentage ,location,font of text,length,rgb color,thickness\n",
    "    cv2.imshow('Image',img) #Show the video \n",
    "    if cv2.waitKey(1) & 0xff==ord(' '): #By using spacebar delay will stop\n",
    "        break\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "cap.release()     #stop cam       \n",
    "cv2.destroyAllWindows() #close window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img align = \"middle\" src = 'Images/VolumneOutput1.png' width = '700' height = '500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
