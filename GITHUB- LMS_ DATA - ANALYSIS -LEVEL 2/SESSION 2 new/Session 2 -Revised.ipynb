{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2 - MediaPipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOC : <br>\n",
    "\n",
    "1. Overview of MediaPipe and its Capabilities\n",
    "2. Comparison with other computer vision libraries and frameworks\n",
    "3. Installation and Setup\n",
    "4. MediaPipe Hand tracking and Gesture recognition\n",
    "5. MediaPipe Face Detection and Tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview of MediaPipe and its Capabilities\n",
    "\n",
    "MediaPipe is an open-source cross-platform framework developed by Google for building multimodal machine learning applications. It provides a wide range of pre-built modules for tasks like face detection, pose estimation, hand tracking, object detection, and more. MediaPipe is built using C++ and Python and can be used on multiple platforms like Android, iOS, Windows, and Linux. MediaPipe also provides APIs for integrating custom machine learning models into the pipeline.\n",
    "\n",
    "Mediapipe is a powerful library that provides a wide range of computer vision and machine learning solutions. Here are some of the things that can be done using Mediapipe:\n",
    "\n",
    "- Object detection and tracking: Detecting and tracking objects in video or images.\n",
    "- Face detection and recognition: Detecting faces and recognizing faces in video or images.\n",
    "- Hand tracking and gesture recognition: Detecting and tracking hands and recognizing gestures in real-time video or images.\n",
    "- Pose estimation: Estimating the human body pose from images or video streams.\n",
    "- Segmentation: Segmenting objects from images or video streams.\n",
    "Image and video processing: Various image and video processing operations such as resizing, cropping, rotation, filtering, and blending.\n",
    "- Audio processing: Processing and analyzing audio signals for various applications such as speech recognition, speaker identification, and emotion detection.\n",
    "- Natural Language Processing (NLP): Natural language processing tasks such as sentiment analysis, text classification, and speech-to-text conversion.\n",
    "These are just a few examples of the many things that can be done using Mediapipe. The library is constantly evolving and new features are added frequently, making it an extremely versatile and powerful tool for computer vision and machine learning applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comparison with other computer vision libraries and frameworks\n",
    "\n",
    "MediaPipe provides a unique combination of machine learning-based approaches and traditional computer vision techniques that make it stand out from other computer vision libraries like OpenCV and frameworks like TensorFlow. MediaPipe provides a pipeline for building multimodal applications that integrate multiple machine learning and computer vision techniques. It also provides pre-built modules that can be used out of the box, reducing the need for complex code development."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Installation and Setup\n",
    "\n",
    "MediaPipe can be installed using pip, the Python package manager, as follows:\n",
    "\n",
    "```pip install mediapipe```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MediaPipe Hand tracking and Gesture recognition\n",
    "\n",
    "One of the most popular features of Mediapipe is the hand landmark detection module, which allows for real-time and accurate detection of 21 key points (landmarks) on a person's hand.\n",
    "\n",
    "The hand landmark detection module uses a deep neural network to analyze an input image or video frame and predict the 3D coordinates of the 21 hand landmarks. These landmarks correspond to various points on the hand, such as the tips of the fingers, the base of the thumb, and the center of the palm.\n",
    "\n",
    "The Mediapipe hand landmark detection pipeline is composed of several stages, including:\n",
    "\n",
    "- Hand detection: The first step involves detecting the presence of a hand in the input image or video frame. This is done using a machine learning model that has been trained to recognize the shape and structure of a hand.\n",
    "\n",
    "- Hand localization: Once a hand has been detected, the next step involves localizing the hand and aligning it to a canonical coordinate system. This is important for ensuring that the hand landmarks are consistently detected across different orientations and positions of the hand.\n",
    "\n",
    "- Hand landmark estimation: The final stage involves estimating the 3D coordinates of the 21 hand landmarks. This is done using a deep neural network that has been trained on a large dataset of hand images and corresponding landmark annotations.\n",
    "\n",
    "Once the hand landmarks have been detected, they can be used for a wide range of applications, such as gesture recognition, hand tracking, and virtual try-on. The Mediapipe hand landmark detection module is highly optimized for real-time performance and can be easily integrated into Python applications using the Mediapipe Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load the Mediapipe hand landmark model\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame from the video capture object\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the color space from BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect the hand landmarks in the current frame\n",
    "    results = mp_hands.process(frame)\n",
    "\n",
    "    # Draw the hand landmarks on the current frame\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the current frame in a window\n",
    "    cv2.imshow('Hand Landmarks', frame)\n",
    "\n",
    "    # Check for a key event and exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MediaPipe Face Detection and Tracking\n",
    "\n",
    "- MediaPipe Face Detection and Tracking is a pre-built computer vision pipeline developed by Google that uses machine learning to detect and track faces in real-time video streams or image sequences. It is based on a deep neural network trained on a large dataset of images and is capable of detecting and tracking multiple faces simultaneously.\n",
    "\n",
    "- The MediaPipe Face Detection and Tracking pipeline consists of two main components: a face detection model and a face tracking model. The face detection model is responsible for detecting faces in the input video frames or images, while the face tracking model is responsible for tracking the detected faces across frames and maintaining their identities.\n",
    "\n",
    "- The face detection model is based on the Single Shot Detector (SSD) architecture, which is a popular object detection algorithm that uses a single neural network to predict object bounding boxes and class probabilities in an input image. The SSD architecture is trained on a large dataset of annotated images of faces and is capable of detecting faces in various orientations and lighting conditions.\n",
    "\n",
    "- The MediaPipe Face Detection and Tracking pipeline can be used for a wide range of applications, including video conferencing, virtual makeup try-on, and emotion detection. It is also highly customizable, allowing developers to fine-tune the pipeline for specific use cases and integrate it into their own applications.\n",
    "\n",
    "Here's an example code snippet to detect and track faces using MediaPipe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize the MediaPipe face detection module\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# Initialize the MediaPipe drawing module\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the VideoCapture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop over the frames\n",
    "while True:\n",
    "    # Read the frame from the camera\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        results = face_detection.process(img_rgb)\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Draw the bounding box around the face\n",
    "                mp_draw.draw_detection(img, detection)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow(\"Face Detection\", img)\n",
    "\n",
    "    # Wait for a key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and destroy the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
